---
type: breadcrumb
title: "Think: provider integration test v1.1.1"
topic: provider integration test v1.1.1
status: active
created: "2026-01-26T14:11:34.729Z"
updated: "2026-01-26T15:16:50.757Z"
tags:
  - think
  - active
scope: project
---

# provider integration test v1.1.1

_Thinking document created 2026-01-26T14:11:34.729Z_

## Thoughts

### 2026-01-26T14:11:40.113Z - Thought
Provider routing tests should pin to an explicit decision matrix (capability + policy + priority/weight rules) rather than live provider health, or they’ll flake when quotas/latency shift. Verify failover semantics too: which error classes trigger reroute, how retry budgets and stickiness are applied, and that identical inputs yield the same chosen provider.

### 2026-01-26T14:12:34.951Z - Thought (model:gpt-5.2-codex)
Locking routing tests to a fixed decision matrix risks masking real-world regressions: if live health, quota throttling, or latency spikes would alter routing, a “pinned” test can pass while production behavior degrades. Consider complementing deterministic unit tests with a small, controlled integration suite that exercises actual health signals (in a sandbox with stable quotas) to validate that dynamic inputs do influence routing as intended, especially under partial outage or brownout conditions.

### 2026-01-26T14:13:32.718Z - Thought (model:unknown)
A hybrid approach using recorded interaction replay (VCR-style) offers the best balance. It allows us to simulate specific "live" conditions—like high latency or 429 Rate Limit responses—deterministically, ensuring the failover logic triggers correctly without relying on actual external volatility. Furthermore, we must verify the *normalization layer* guarantees that swapping providers (e.g., falling back from a high-intelligence model to a faster/cheaper one) maintains a consistent output schema for the application, preventing downstream crashes during valid failover events.

### 2026-01-26T14:20:39.162Z - Thought (model:haiku [4e415334-1460-4221-ae3a-27c486271347])
Recording interaction replay via VCR-style fixtures is pragmatic, but you're glossing over a critical implementation detail: the normalization layer needs to handle not just schema consistency, but *semantic drift* across provider substitutions. A Claude model returning structured reasoning might include `thinking` fields that a Gemini model lacks entirely; swapping providers mid-stream forces you to either strip semantic richness (losing debug data) or fabricate placeholder fields (corrupting the output). The real win isn't just ensuring failover *works*—it's documenting which capabilities are *truly interchangeable* versus which ones represent meaningful degradation. That requires a capability matrix that's explicitly versioned alongside your recorded fixtures, so when Provider B adds a new feature, your tests don't silently start exercising a code path that only works with Provider A's extended schema. Without that explicitness, you've just moved the flakiness from "latency spikes" to "subtle semantic incompatibilities that manifest as weird bugs three weeks after a provider updates their API."

### 2026-01-26T15:16:50.757Z - Thought (model:gpt-5.1-codex-max)
Add a contract-level “capability delta” check that runs whenever a provider client is bumped: compare recorded fixture outputs against a semantic schema diff (e.g., expected fields, nullability, enum domains, tokenization quirks) and fail fast if any drift would invalidate normalization assumptions. Pair this with chaos-style replay that injects mixed-mode streams (partial tool calls, truncated responses, mixed audio/text tokens) to ensure the router’s stickiness, retry budget, and downgrade paths don’t just pick a provider deterministically but also preserve end-to-end invariants like idempotent function invocation and stable cost accounting under degradation.
