# Exploration: v1.1.0 Features - Enhanced Hints, Auto-Selection, and Memory Injection

**Created**: 2026-01-24
**Explorer**: Claude Sonnet 4.5
**Status**: Ready for Specification
**Deliberation**: decision-v110-feature-exploration-hint-visibility-auto-flag-enhanced-injection

---

## Feature Intent

v1.1.0 introduces four features to improve the user experience of the memory plugin:

1. **Enhanced Hint Visibility**: Make CLI hints more discoverable through progressive disclosure, stderr output, and interactive prompts for complex deliberations
2. **--auto Flag**: Intelligent agent/style/model selection using local Ollama for AI-assisted deliberation with transparency and user confirmation
3. **Enhanced Memory Injection**: Extend contextual memory injection beyond gotchas to include decisions and learnings, with user-configurable relevance thresholds
4. **Cross-Provider Agent Calling**: Support `--call codex` and `--call gemini` alongside Claude, with `--model` as the only portable parameter (Codex also supports `--oss` for local models)

These features address usability gaps where valuable functionality (AI-assisted thinking, contextual memory surfacing, provider flexibility) exists but is under-discovered or under-utilised.

---

## Suggested Specify Prompt

Use this as the argument for `/speckit:specify`:

```
Feature: Enhanced CLI Hints, Auto-Selection, and Memory Injection for v1.1.0

ACTORS:
- CLI users running memory think commands
- Developers reading/editing files with relevant memory context
- Plugin administrators configuring memory injection behaviour

FEATURE 1: Enhanced Hint Visibility

WHAT: Improve discoverability of CLI hints for memory think --call functionality
- Output hints to stderr (not JSON stdout) to separate informational messages from data
- Implement progressive disclosure: show hints first N uses, then reduce frequency
- Add interactive confirmation prompts when thought content is complex (>200 chars or contains questions)
- Enhance --help output with concrete examples of --call, --style, --agent usage

WHY: Current hint system outputs to JSON, making it invisible to users in normal CLI flow. Users are unaware of powerful --call claude functionality that enables AI-assisted deliberation.

SUCCESS CRITERIA:
- Hints appear in terminal output (stderr) separate from JSON responses
- Hint display count tracked per session, with configurable threshold (default: first 3 uses)
- Interactive prompts trigger for complex thoughts, asking user to confirm AI invocation
- Help text includes at least 3 concrete examples of AI-assisted think usage
- Pipeline usage preserved via --non-interactive flag that suppresses prompts


FEATURE 2: --auto Flag for Intelligent Selection

WHAT: Add --auto flag to memory think commands that uses a tiered selection strategy:
- **Tier 1 (Ollama)**: If available, use local LLM to analyse thought topic and recommend agent/style/model
- **Tier 2 (Heuristics)**: If Ollama unavailable/timeout, use keyword-based heuristic matching
- Display recommendation with reasoning before invocation: "Auto-selected: --style Devils-Advocate (reasoning: challenging assumptions for counter-argument)"
- Prompt user for confirmation before proceeding with selection

HEURISTIC FALLBACK RULES (when Ollama unavailable):
- Keywords: "security", "vulnerability", "auth" ‚Üí style: Security-Auditor
- Keywords: "architecture", "design", "structure" ‚Üí style: Architect
- Keywords: "should we", "trade-off", "pros cons" ‚Üí style: Devils-Advocate
- Keywords: "performance", "optimise", "speed" ‚Üí style: Pragmatist
- Keywords: "test", "coverage", "spec" ‚Üí agent: test-quality-expert
- Default (no keyword match): style: Architect, model: haiku

DIVERSITY BEHAVIOUR: Repeated --auto calls on the same deliberation return different appropriate selections:
- First call: Pragmatist (appropriate for trade-off question)
- Second call: Architect (also appropriate, but different perspective)
- Third call: Devils-Advocate (challenges assumptions)

Implementation: Pass "Avoid: X, Y (already used)" based on existing thought attributions.

IMPORTANT: Temperature does NOT create diversity. Tested gemma3:1b with temperatures 0.7-1.2 - all returned identical results (Pragmatist 15/15 times). The avoid list is REQUIRED for variety, not optional. This is predictable: first call picks "best" match, subsequent calls with avoid list pick next-best appropriate option.

PERFORMANCE NOTE: With proper prompt structure, gemma3:1b responds in ~0.6s. Larger models (4b+) may take longer on first call (cold start). The 15s timeout accommodates slower hardware.

WHY: Users must manually discover and remember 15+ output styles and multiple agents. Selection burden reduces usage of valuable deliberation perspectives.

SUCCESS CRITERIA:
- memory think add "..." --auto successfully invokes Ollama for selection (if available)
- If Ollama unavailable or times out (15s), heuristic fallback provides instant selection
- Selection prompt displays recommended agent/style/model with reasoning (source: "ollama" or "heuristics")
- User can approve (y), reject (n), or modify selection before invocation
- Clear warning displayed: "Note: Local model analysis may take 10-15 seconds on typical hardware"
- --non-interactive mode bypasses confirmation and uses recommendation directly
- Security: sanitised inputs, validated selections against discovery results, timeouts enforced


FEATURE 3: Enhanced Memory Injection

WHAT: Extend PostToolUse hook memory injection to include decisions and learnings, not just gotchas
- Configurable via memory.local.md with per-type thresholds and limits
- Different relevance thresholds for gotchas (0.2), decisions (0.35), learnings (0.4)
- Hook-type-specific multipliers (Read: 1.0x, Edit/Write: 0.8x, Bash: 1.2x)
- Conservative defaults (gotchas only), opt-in for decisions/learnings

WHY: Contextual memory surfacing currently only shows gotchas. Relevant decisions and learnings exist but aren't surfaced when reading/editing files, leading to missed context and potential rework.

SUCCESS CRITERIA:
- Reading a file surfaces gotchas (default), plus decisions/learnings (if enabled in config)
- Memory injection respects per-type limits: gotchas (5), decisions (3), learnings (2)
- Hook-type multipliers adjust thresholds: Bash commands have stricter threshold (1.2x), edits have looser threshold (0.8x)
- Session caching prevents duplicate injections per memory type
- Default configuration preserves existing behaviour (gotchas only)
- Example memory.local.md configuration documented with clear opt-in instructions
- Performance: single semantic search call, client-side filtering by type

CONSTRAINTS:
- Must preserve pipeline composability (--non-interactive flag)
- Must maintain graceful degradation when Ollama unavailable
- Must avoid overwhelming users with injection noise (hence opt-in and limits)
- Must respect existing architectural patterns: library-first design, settings via memory.local.md
- Performance budget: hook latency <500ms for typical cases

ASSUMPTIONS:
- Ollama is available locally (fallback exists if not)
- Users have memory.local.md for configuration (graceful defaults if missing)
- Session state/cache mechanisms can track hint display counts and injected memories


FEATURE 4: Cross-Provider Agent Calling (Codex & Gemini)

WHAT: Extend memory think --call to support Codex and Gemini CLIs alongside Claude
- Support `--call codex` and `--call gemini` in addition to `--call claude`
- Only `--model` parameter is portable across providers
- Codex additionally supports `--oss` flag for local models (gpt:oss-20b, gpt:oss-120b)
- Fail gracefully if provider CLI is not installed

WHY: Users may prefer different AI providers for different deliberation tasks. Codex offers OSS local model support (no subscription needed). Gemini offers alternative perspectives. Cross-provider calling proves the plugin is provider-agnostic.

CLI COMMAND STRUCTURES (discovered via --help):
- Claude: `claude --print "prompt" --model sonnet`
- Codex: `codex exec "prompt" --model gpt-5-codex` or `codex exec "prompt" --model gpt:oss-20b --oss`
- Gemini: `gemini "prompt" --model gemini-2.5-pro -o text`

SUCCESS CRITERIA:
- `memory think add "Topic" --call codex --model gpt-5-codex` invokes Codex CLI
- `memory think add "Topic" --call codex --model gpt:oss-20b --oss` uses local Codex model
- `memory think add "Topic" --call gemini --model gemini-2.5-pro` invokes Gemini CLI
- If CLI not installed: graceful error with install instructions
- Response attributed correctly: "by: model:gpt-5-codex provider:codex [session-id]"
- Output parsing handles provider-specific noise (Codex headers, Gemini extension loading)

DEFERRED TO v1.2.0:
- --style via prompt injection (works universally but quality varies)
- Provider-specific params: --profile (Codex), --extensions (Gemini), --agent (Claude-only)
- Gemma local model support (pending tool-call capability)

CONSTRAINTS:
- Keep v1.1.0 minimal: only --model and --oss (Codex) are supported
- No abstraction layer attempting to unify provider concepts (profiles ‚â† extensions ‚â† agents)
- Require explicit provider selection (no auto-detection)
```

---

## Suggested Plan Prompt

Use this as the argument for `/speckit:plan`:

```
Technology and Architecture for v1.1.0 Features

LANGUAGE & RUNTIME:
- TypeScript with Bun runtime (existing stack)
- Use existing ollama-js library integration in hooks/src/services/ollama.ts
- Leverage existing session cache patterns from hooks/src/session/session-cache.ts

LIBRARIES & DEPENDENCIES:
- prompts (npm): Interactive CLI prompts with stdin/stdout/stderr control
  - Rationale: Type-safe, well-maintained, supports non-interactive mode
- Extend existing: discovery.ts (agent/style enumeration), plugin-settings.ts (config parsing)
- NO new dependencies for hint tracking (use existing session cache pattern)

ARCHITECTURAL APPROACH:
Follow library-first design principle per project constitution:
- Direct use of prompts package, no wrapper abstractions
- Extend existing Ollama service (ollama.ts), don't create new client layers
- Reuse session cache patterns, don't create new tracking mechanisms

FEATURE 1 IMPLEMENTATION:
File: skills/memory/src/cli/response.ts
- Modify outputResponse() to write hint to stderr before JSON to stdout
- Use process.stderr.write() for hint output (not console.error which adds formatting)
- Ensure stderr flush completes before stdout JSON (ordering guarantee)

File: skills/memory/src/cli/hint-tracker.ts (NEW)
- Track hint display count per hint type using session state
- loadHintState(sessionId, hintType) ‚Üí count
- incrementHintCount(sessionId, hintType) ‚Üí newCount
- shouldShowHint(sessionId, hintType, threshold=3) ‚Üí boolean
- Store in .claude/session-state/<sessionId>/hints.json

File: skills/memory/src/cli/commands/think.ts
- Import prompts package for interactive confirmation
- Check thought length/content for complexity before --call invocation
- Prompt: "This thought seems complex. Invoke AI for assistance? (y/N)"
- Respect --non-interactive flag: skip all prompts if set
- Update help text with examples section

FEATURE 2 IMPLEMENTATION:
File: skills/memory/src/think/auto-selector.ts (NEW)
- HEURISTIC_RULES: Map<RegExp[], {style?, agent?, model?}> - keyword patterns to selections
  - /security|vuln|auth|injection/i ‚Üí style: Security-Auditor
  - /architect|design|structure|pattern/i ‚Üí style: Architect
  - /should we|trade-?off|pros.?cons|versus|vs\b/i ‚Üí style: Devils-Advocate
  - /perform|optimi[sz]e|speed|latency|slow/i ‚Üí style: Pragmatist
  - /test|coverage|spec|assert/i ‚Üí agent: test-quality-expert
  - Default: style: Architect, model: haiku
- selectViaHeuristics(topic, thoughtContent) ‚Üí {style?, agent?, model?, reasoning, source: 'heuristics'}
- buildSelectionPrompt(topic, thoughtType, availableAgents, availableStyles, availableModels, usedStyles) ‚Üí prompt
  - MUST include full list of available styles with descriptions
  - MUST include full list of available agents with descriptions
  - MUST include available Claude models (haiku, sonnet, opus)
  - MUST include "Avoid: X, Y (already used)" for styles already in deliberation
  - Format: structured prompt that constrains model to pick from provided options
  - Benefit: Repeated --auto calls build diverse deliberation naturally
- parseSelectionResponse(ollamaOutput) ‚Üí {agent?, style?, model?, reasoning} | null
  - Handle malformed JSON (smaller models ramble)
  - Extract first valid JSON object from response
  - Validate extracted values against available options
  - Return null if parsing fails (triggers heuristic fallback)
- selectAgentStyleModel(topic, thoughtType, thoughts, basePath, options) ‚Üí AICallOptions
  - Read chat model from memory.local.md (user controls speed vs quality)
  - Call discoverAgents() and discoverStyles() from discovery.ts
  - Build prompt with discovered options
  - Try Ollama with 15s timeout and spinner ("Analysing thought...")
  - If Ollama fails/times out/returns unparseable: fall back to selectViaHeuristics()
  - Validate agent/style names against discovery results (security: whitelist)
  - Return structured selection with source indicator ('ollama' | 'heuristics')

File: skills/memory/src/cli/commands/think.ts
- Handle --auto flag in thinkAdd()
- Display warning: "Note: Local model analysis may take 10-15 seconds on typical hardware"
- If --auto: call selectAgentStyleModel(), display recommendation with source
- Format: "Auto-selected (via ollama): --style X --agent Y. Proceed? (Y/n)"
- Or: "Auto-selected (via heuristics - ollama unavailable): --style X. Proceed? (Y/n)"
- On approval: merge selection into call options
- On rejection: prompt user for manual selection or abort

Security considerations:
- Sanitise thought content before inclusion in Ollama prompt (escape special chars)
- Validate selection against discovery results (prevent injection of non-existent agents)
- Timeout enforcement: 15s for Ollama call (accommodates slower hardware)
- Circuit breaker: after 3 consecutive Ollama failures, skip to heuristics for session
- Show full agent/style path in confirmation (transparency about scope: local/plugin/global/enterprise)

FEATURE 3 IMPLEMENTATION:
File: hooks/src/memory/enhanced-injector.ts (NEW, extends gotcha-injector.ts)
- getRelevantMemories(types: MemoryType[], filePath, contextTags, basePath, config) ‚Üí MemoryResult[]
  - Single searchMemories() call with combined query
  - Client-side filtering by type with per-type thresholds
  - Apply hook-type multiplier to thresholds
  - Sort by type priority (gotcha > decision > learning) then relevance score
  - Limit per type: gotchas (5), decisions (3), learnings (2)

File: hooks/src/settings/injection-settings.ts (NEW)
- loadInjectionConfig(basePath) ‚Üí InjectionConfig
- Parse memory.local.md for injection settings
- Schema: { enabled, types: {gotcha: {enabled, threshold, limit}, ...}, hook_tiers: {...} }
- Provide sensible defaults matching current behaviour (gotchas only)

File: hooks/post-tool-use/memory-context.ts
- Import enhanced-injector instead of gotcha-injector
- Load injection config from settings
- Determine hook type from input.tool_name
- Apply hook multiplier to thresholds
- Call getRelevantMemories() with enabled types
- Format output with type-specific icons: üö® Gotchas, üìã Decisions, üí° Learnings
- Session cache keyed by (memoryId, type) to prevent duplicates

File: .claude/memory.example.md (UPDATE)
- Add injection configuration example with comments
- Document opt-in process for decisions/learnings
- Explain threshold multipliers and their use cases

Performance optimisation:
- Lazy loading: inject gotchas immediately, decisions/learnings only if no critical gotchas
- Cache semantic search results per (filePath, hookType, sessionId) for 2 minutes
- Hard limit total injected memories to 10 regardless of configuration

FEATURE 4 IMPLEMENTATION:
File: skills/memory/src/think/providers.ts (NEW)
- ProviderConfig interface: { command, modelFlag, extraFlags?, outputParser }
- PROVIDERS map:
  - claude: { command: 'claude', args: ['--print'], modelFlag: '--model', supportsAgent: true, supportsStyle: true }
  - codex: { command: 'codex', args: ['exec'], modelFlag: '--model', extraFlags: { oss: '--oss' } }
  - gemini: { command: 'gemini', args: [], modelFlag: '--model', extraArgs: ['-o', 'text'] }
- isProviderInstalled(provider) ‚Üí boolean (check with `which` or similar)
- buildProviderCommand(provider, prompt, options) ‚Üí string[]
- parseProviderOutput(provider, rawOutput) ‚Üí string
  - Claude: return as-is (--print is clean)
  - Codex: strip header lines (version, workdir info)
  - Gemini: filter extension loading noise

File: skills/memory/src/think/ai-invoke.ts (MODIFY)
- Import providers.ts
- Add `provider` field to AICallOptions (default: 'claude')
- In invokeAI(): check provider, route to appropriate CLI
- Build command using buildProviderCommand()
- Parse output using parseProviderOutput()
- Update attribution: "model:X provider:Y [session-id]"

File: skills/memory/src/cli/commands/think.ts (MODIFY)
- Parse --call value as provider name (claude|codex|gemini)
- Parse --oss flag (only valid with --call codex)
- Validate: if --agent/--style with non-claude provider, warn and ignore
- Pass provider to invokeAI()

Error handling:
- If provider CLI not found: "Codex CLI not installed. Install with: npm i -g @openai/codex"
- If --oss used without codex: "The --oss flag is only supported with --call codex"
- If --agent used with codex/gemini: "Warning: --agent is only supported with --call claude (ignored)"

TESTING STRATEGY:
- Unit tests for hint-tracker (session state persistence)
- Unit tests for auto-selector (prompt building, response parsing, validation)
- Unit tests for enhanced-injector (multi-type filtering, threshold application)
- Unit tests for providers.ts (command building, output parsing, provider detection)
- Integration test: think add with --auto, verify Ollama invocation and confirmation
- Integration test: Read hook with enhanced injection config, verify multi-type surfacing
- Integration test: think add --call codex, verify Codex CLI invocation (mock or real)
- Integration test: think add --call gemini, verify Gemini CLI invocation (mock or real)
- Error handling test: --call codex when Codex not installed ‚Üí graceful error
- Manual testing: --non-interactive flag suppresses all prompts
- Manual testing: Ollama timeout/unavailable handling

IMPLEMENTATION PHASES:
Phase 1: Feature 1 (Hint Visibility) - lowest risk, highest immediate value
  - Implement stderr output in response.ts
  - Add hint-tracker.ts with session state
  - Update think.ts help text
  - Add interactive prompts with --non-interactive flag

Phase 2: Feature 3 (Enhanced Injection) - builds on existing patterns
  - Extend injection configuration in memory.local.md
  - Implement enhanced-injector.ts with multi-type support
  - Update PostToolUse hook with new injector
  - Document opt-in configuration

Phase 3: Feature 2 (--auto Flag) - most complex, needs security hardening
  - Implement auto-selector.ts with Ollama integration
  - Add selection prompt and confirmation flow
  - Implement security safeguards (validation, timeouts, sanitisation)
  - Integration testing with discovery system

Phase 4: Feature 4 (Cross-Provider Calling) - moderate complexity, external dependencies
  - Implement providers.ts with provider configs and output parsers
  - Modify ai-invoke.ts to route to different CLIs
  - Add --oss flag support for Codex local models
  - Error handling for missing CLIs
  - Integration testing with mocked provider CLIs

ROLLOUT PLAN:
- Default behaviour unchanged (backwards compatible)
- Hints appear progressively (first 3 uses by default)
- Enhanced injection disabled by default (opt-in via config)
- --auto flag optional (users continue manual selection)
- Documentation updated with migration guide and examples
```

---

## Research Notes

### Architectural Approach

**Recommended**: Library-first, extend existing patterns, avoid new abstractions

**Rationale**: Project constitution emphasises "anti-abstraction principle" and "library-first design". All three features extend existing mechanisms rather than introducing new layers.

**Alternatives Considered**:
- **Custom hint renderer**: Create abstraction layer for hint output ‚Üí Rejected: adds unnecessary complexity, direct stderr.write() sufficient
- **Dedicated auto-selection service**: Create new service layer for AI routing ‚Üí Rejected: extends existing ollama.ts service, no new layer needed
- **Plugin-based injection system**: Create pluggable injector architecture ‚Üí Rejected: over-engineered for 3 memory types, direct extension of gotcha-injector sufficient

### Technology Evaluation

**Recommended Libraries/Frameworks**:
- **prompts**: Interactive CLI prompts - Rationale: Type-safe, 1M+ weekly downloads, supports non-interactive mode natively
- **ollama-js** (existing): Already integrated, no additional dependency needed

**Alternatives**:
- **inquirer**: More features but heavier, less TypeScript-friendly ‚Üí Rejected: prompts is lighter and sufficient
- **Custom prompt implementation**: Build stdin/stdout handling manually ‚Üí Rejected: reinvents wheel, prompts is battle-tested

### Key Considerations

**Performance**:
- Single semantic search per hook invocation (not 3 separate searches)
- Session caching prevents redundant Ollama calls
- Lazy loading defers decisions/learnings until after gotcha check
- Estimated latency: +50-100ms for enhanced injection

**Ollama Benchmark Results** (tested 2026-01-24):

*Initial tests with vague prompts:*
| Model | Response Time | Quality |
|-------|--------------|---------|
| gemma3:1b | ~2.5s | ‚ö†Ô∏è Rambled, malformed JSON |
| gemma3:4b | ~10s (cold) | ‚úÖ Clean JSON |

*Revised tests with explicit prompt structure ("Respond ONLY with valid JSON"):*
| Model | Response Time | Quality | Notes |
|-------|--------------|---------|-------|
| gemma3:270m | ~1.4s | ‚ùå Echoes placeholders literally | Too small for task |
| gemma3:1b | **~0.6s** | ‚úÖ Clean JSON, correct selections | **Recommended default** |
| gemma3:4b | ~1.1s | ‚úÖ Clean JSON, correct selections | Slightly slower, same quality |

- **Key insight**: Prompt engineering matters more than model size! Explicit constraints yield reliable results from 1b
- **Tested selections**: Security-Auditor for SQL injection ‚úÖ, Architect for module structure ‚úÖ, Architect for performance ‚úÖ
- **Recommendation**: Use gemma3:1b as default for --auto (fast + reliable with proper prompt)
- **Implementation must**: Use explicit prompt format with "Respond ONLY with valid JSON" constraint
- **Fallback**: Heuristics still needed for 270m-class models or parse failures
- **UX**: No spinner warning needed for 1b (~0.6s is imperceptible); warn only for larger models

**Security**:
- Prompt injection mitigation: sanitise thought content before Ollama prompts
- Selection validation: whitelist against discovery results
- Timeout enforcement: 15s maximum for Ollama calls (accommodates typical hardware)
- Circuit breaker: skip Ollama after 3 consecutive failures in session
- Transparency: show full agent/style origin (local vs enterprise) and selection source (ollama vs heuristics)

**Complexity**:
- Feature 1: Low complexity, direct extension of existing response system
- Feature 3: Medium complexity, extends established pattern (gotcha-injector)
- Feature 2: High complexity, involves AI reasoning, security concerns, user interaction

**Dependencies**:
- prompts: 1 new dependency (well-established, minimal footprint)
- Ollama: optional dependency (graceful degradation already implemented)

### Open Questions

1. ~~Should hints persist across sessions or reset per session?~~ ‚Üí **Decision**: Per-session tracking, reset on /clear (aligns with session-state pattern)
2. ~~What confidence threshold for auto-selection acceptance?~~ ‚Üí **Decision**: Always prompt user, no auto-acceptance (security over convenience)
3. ~~Should enhanced injection apply to all hooks or just PostToolUse?~~ ‚Üí **Decision**: PostToolUse only (Read/Edit/Write hooks), PreToolUse is for blocking operations

---

## Next Steps

1. Review the suggested prompts above
2. Adjust if needed based on project-specific context
3. Run `/speckit:specify` with the suggested specify prompt
4. Run `/speckit:plan` with the suggested plan prompt
5. Use memory search to retrieve research notes during planning (decision-v110-feature-exploration-hint-visibility-auto-flag-enhanced-injection)

---

## Sources

Research conducted using:
- **Codebase Analysis**: hooks/src/services/ollama.ts, skills/memory/src/cli/response.ts, hooks/src/memory/gotcha-injector.ts, skills/memory/src/think/ai-invoke.ts, skills/memory/src/think/discovery.ts
- **Web Research**:
  - [GitHub - terkelg/prompts](https://github.com/terkelg/prompts) - Interactive CLI prompts library
  - [Building CLI apps with TypeScript in 2026](https://dev.to/hongminhee/building-cli-apps-with-typescript-in-2026-5c9d) - Modern CLI best practices
  - [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) - Ollama chat/generate API reference
  - [Ollama Chat - Spring AI Reference](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html) - Function calling patterns
  - [Progressive Disclosure Matters: Applying 90s UX Wisdom to 2026 AI Agents](https://aipositive.substack.com/p/progressive-disclosure-matters) - Progressive disclosure principles
  - [Progressive Disclosure - NN/G](https://www.nngroup.com/articles/progressive-disclosure/) - User experience best practices
- **Memory Search**: decision-plugin-settings-design-for-memory-plugin, decision-output-styles-for-memory-assessment-agents, learning-semantic-search-embedding-caching-success

## AI Deliberation Summary

Three AI perspectives consulted during exploration:

1. **Architect (via --style Architect)**: Surfaced stderr/stdout ordering concerns, scope boundary issues for session-state tracking, and composability risks with interactive prompts. Recommended --non-interactive flag and questioned whether all three sub-features should be coupled.

2. **Security-Auditor (via --style Security-Auditor)**: Identified prompt injection vulnerabilities, trust boundary issues with agent discovery, model output exploitation risks, and latency-based DoS vectors. Recommended input sanitisation, selection validation, timeouts, and explicit user confirmation gates.

3. **Pragmatist (via --style Pragmatist)**: Questioned whether decisions/learnings need injection at all, raised performance concerns about multiple semantic searches, and suggested focusing gotcha injection improvements over expanding to new types. Advocated for batch caching and hard limits.

**Synthesis**: Balance innovation with pragmatism. Implement all three features with conservative defaults (opt-in for advanced functionality), security safeguards (validation, timeouts, sanitisation), and performance optimisations (single search, client-side filtering, caching). Preserve pipeline composability via --non-interactive flag.
