---
type: breadcrumb
title: "Think: TDD Parity Gap Investigation: Is 97.4% Good Enough?"
topic: "TDD Parity Gap Investigation: Is 97.4% Good Enough?"
status: concluded
created: "2026-01-14T08:16:01.971Z"
updated: "2026-01-14T08:24:08.928Z"
tags:
  - think
  - concluded
scope: project
conclusion: The 97.4% gap warrants brief investigation (30 min max) to identify the 2 phantom functions. If they're parser limitations, document them; if real gaps, add tests. The TDD parity tool is useful as a signal, not gospel - understanding what we're measuring matters more than hitting 100%.
promotedTo: learning-tdd-parity-gap-investigation-is-97-4-good-enough
---

# TDD Parity Gap Investigation: Is 97.4% Good Enough?

_Thinking document created 2026-01-14T08:16:01.971Z_

## Thoughts

### 2026-01-14T08:16:18.592Z - Thought
Initial observation: The hooks parity report shows a contradiction - claims 'all public functions have some test coverage' but reports 76/78 (97.4%). This 2-function gap could be: (1) a real coverage gap, (2) a TypeScript regex parser limitation, or (3) a counting quirk in how the tool handles class methods vs standalone functions.

### 2026-01-14T08:17:52.335Z - Thought (claude:haiku [ae3a8784-588e-468c-a975-b9e9199b834f])
The contradiction you've identified is worth unpacking more carefully. The "all public functions have some test coverage" message is likely checking presence of *any* test reference (even a stub or skip), while the 76/78 count is stricter - probably requiring actual executable test assertions.

This matters because it determines whether that 2.6% gap is:
- **Noise**: Two functions have test scaffolding but the tool's regex can't parse the actual test body, so it marks them untested even though they're covered
- **Real risk**: Two exported functions genuinely have no test contact, just a coincidence that the discovery logic says otherwise

Quick diagnostic: Check if those 2 missing functions are in class methods or have unusual signatures (generics, overloads, etc). Regex parsers choke on those. If they're simple exported functions with straightforward signatures and genuinely no test files importing them, that's a real gap worth fixing.

But here's the pragmatic take: 97.4% with a regex-based tool detecting coverage is effectively "good enough" for merge-blocking purposes. The tool's limitations are now documented, so you know where false positives live. Use it as a signal (98% is healthy), not gospel (that 2.6% might be noise). If those 2 functions are critical path code, add tests. If they're utilities or helpers, the risk-reward doesn't justify the effort.

### 2026-01-14T08:18:14.702Z - Counter-argument (claude:haiku [4dfab8e2-7423-4025-af95-661993bc9ca7])
Accepting 97.4% creates a false standard that will haunt you later. Here's why this matters:

First, the "good enough" argument is a trap. If you're building tooling that enforces TDD parity for other teams, and you ship with 2 untested functions in your own codebase, you've just given every team an excuse to do the same. "Well, Claude's parity tool has gaps too" becomes the justification for cutting corners.

But here's the sharper problem: you don't actually know what those 2 functions are yet. You're accepting the gap without understanding it. What if they're critical path functions? What if they're getters that seem trivial but actually contain validation logic? The regex parser limitation theory sounds plausible, but that's speculation masking investigation.

The real cost isn't the missing tests themselves—it's the uncertainty. You're shipping a tool while explicitly acknowledging you don't fully understand your own coverage. That erodes confidence in the entire system. When someone asks "is this tool reliable?", your answer has to be "mostly, but we have some unexplained gaps".

And there's a maintenance burden hiding here: those 2 phantom functions will become technical debt. Next time someone refactors this code, they'll either rediscover the gap or the gap will silently grow. You're not closing a loop; you're deferring a problem.

The question isn't whether 97.4% is mathematically acceptable—it's whether you're comfortable shipping uncertainty. If the answer is no, you need to either find those 2 functions and test them, or prove they genuinely don't exist and update your tooling to match that reality.

### 2026-01-14T08:18:35.391Z - Alternative (claude:haiku [540b3ffe-1ddf-4985-9add-495a2b89a853])
The gap between "all public functions have some test coverage" and the 76/78 count points to a systematic blind spot in how we're measuring. Rather than accepting 97.4% as "good enough", we should treat this as a debugging problem with specific steps:

**Isolate what's being counted differently:**
Run the parity tool with verbose output to get the actual lists - which 76 functions are counted as covered? Which 2 are missing? This isn't about arbitrary thresholds; it's about understanding whether those 2 functions are legitimately untestable infrastructure code or actual gaps in test coverage.

**Check for parser limitations:**
TypeScript regex parsers often struggle with certain syntax patterns - decorators on class methods, getters/setters, overloaded function signatures, or private functions exposed through re-exports. If the 2 missing functions fall into these categories, the tool isn't measuring correctly, and we need to either fix the parser or document the limitation.

**Verify the "any test reference" logic:**
Write a quick test script that identifies which functions have *any* test mention (even skipped tests or stubs) versus which have actual assertions. The gap between these numbers reveals whether we're looking at incomplete tests or complete gaps.

**Establish a proper baseline:**
Once we know which 2 functions are in the gap, decide: are these functions genuinely untestable (e.g., internal error handlers, setup utilities), or are they public API that should be tested? This decision should be documented, not assumed based on a percentage threshold.

Accepting 97.4% without understanding what it means sets a precedent that metrics matter more than clarity. The investigation itself - understanding *why* we're at 97.4% - is the actual value.

### 2026-01-14T08:21:52.393Z - Thought (claude:haiku (Risk-Assessor) [4bc262fa-9331-4ab4-be83-cde6747f2043])
The 2-function gap warrants investigation because it reveals whether your test infrastructure has a systematic blind spot. If those 2 functions are genuinely untested, 97.4% masks a real risk - the untested functions are statistically more likely to contain bugs. If they're actually tested but the tool can't detect it, you have a different problem: your test naming conventions or structure don't align with the detection heuristics, which means future developers won't know which patterns the tool can see.

I'd recommend a third path: treat this as a data integrity issue, not a threshold question. Spend 30 minutes identifying exactly which 2 functions are being reported as uncovered, manually verify their test status, then use that finding to either (a) write missing tests, or (b) update your test detection logic if the tool is blind to a legitimate pattern you're using. Either way, you'll understand whether 97.4% is a real achievement or a measurement error masquerading as coverage.

The "good enough" question only makes sense *after* you've resolved what the numbers actually mean.

### 2026-01-14T08:22:11.564Z - Thought
Manual thought - no AI invocation. Should have no 'by' field unless I add --by manually.

### 2026-01-14T08:22:18.912Z - Thought (Human tester)
Manual thought with attribution - should show 'by' as 'Human tester'

### 2026-01-14T08:22:36.277Z - Thought (claude:haiku  [65cceb0d-49c8-4ad4-8f69-c5b290a61052])
The real issue here is that you're treating a measurement discrepancy as if it's a philosophical question about acceptable thresholds, when it's actually a debugging problem masquerading as a standards discussion.

Here's what's actually happening: The tool is doing two different things and calling them both "coverage". The first check (all public functions have *some* test coverage) likely has a lower bar—it's scanning for any test file that mentions the function name, including skipped tests, stubs, or tests that don't actually exercise the function. The second count (76/78) is probably doing proper AST parsing and finding only functions where there's genuine test code that would execute.

This matters because you're not at 97.4%—you're at some unknown percentage between 76/78 *and* "all public functions mentioned in tests". The 2-function gap is your actual problem, not your achievement. Those 2 functions are probably edge cases the regex parser is missing or methods the tool categorises differently (maybe private helper functions being counted as public, or vice versa).

Rather than congratulating yourself on 97.4%, you should:
1. Identify exactly which 2 functions are in the gap
2. Determine whether they're *actually* public or if the tool is miscategorising them
3. Add tests if they're legitimately public, or fix the tool's categorisation if they're not

The "good enough" question only makes sense *after* you understand what you're actually measuring. Right now you're sailing with incomplete navigation data.

### 2026-01-14T08:23:34.300Z - Thought (claude:haiku (Concise) [b6308b50-a5eb-4a89-9e7b-1177f02cc968])
The 97.4% figure masks a more important question: are those 2 uncovered functions *actually testable*, or are they utilities/helpers that the tool's regex parser can't properly identify? Before accepting this gap, we need:

1. **Identify the 2 functions** - list them specifically
2. **Assess their criticality** - are they core logic, or infrastructure/helpers?
3. **Check if they're actually public** - the parser might be wrong about what counts as "public"
4. **Determine coverage barriers** - if untestable due to tool limitations, that's different from accepting untested code

The contradiction suggests the tool has asymmetric logic: it's generous in the "all have some coverage" statement (maybe counting any mention) but strict in the 76/78 count (actual test bodies). Fix the measurement first, then decide if the gap is acceptable. A systematic blind spot that affects 2-3% of functions today will expand as the codebase grows.

### 2026-01-14T08:23:55.970Z - Thought (claude:haiku [9a2f1c78-49b2-4183-9489-3c9dd9ec88ae])
The 97.4% figure masks a fundamental asymmetry: the parity tool is measuring *function existence* against *test existence*, not *test coverage quality*. A function could have a test file that imports it, fails to call it, or calls it with stub assertions—all of which would register as 'covered' in a regex scan. The real question isn't whether 2 functions are missing tests, but whether the 76 functions that supposedly have tests are actually being *exercised meaningfully*. This suggests the parity gap is a symptom of a larger measurement blindness rather than a minor oversight.

### 2026-01-14T08:24:08.928Z - Conclusion
The 97.4% gap warrants brief investigation (30 min max) to identify the 2 phantom functions. If they're parser limitations, document them; if real gaps, add tests. The TDD parity tool is useful as a signal, not gospel - understanding what we're measuring matters more than hitting 100%.
