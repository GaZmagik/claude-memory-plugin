---
type: breadcrumb
title: "Think: v1.3.0 Agent Meetings - Scheduled multi-agent deliberation sessions"
topic: v1.3.0 Agent Meetings - Scheduled multi-agent deliberation sessions
status: concluded
created: "2026-01-28T22:33:35.052Z"
updated: "2026-01-30T17:15:10.638Z"
tags:
  - think
  - concluded
scope: project
conclusion: "Agent Meetings concept validated architectural concerns but revealed bigger opportunity: autonomous agent orchestration. Consensus (10 agents): prove value with manual /meeting skill before building scheduling. Key insight: scheduling is easy; orchestration with memory persistence is the innovation. Decision: defer Agent Meetings to new 'Accord' plugin with broader scope - persistent working agents, meeting orchestration, dashboard UI, and local memory per agent. Agent Meetings becomes subset of Accord's capabilities."
promotedTo: decision-v1-3-0-agent-meetings-scheduled-multi-agent-delibe
---

# v1.3.0 Agent Meetings - Scheduled multi-agent deliberation sessions

_Thinking document created 2026-01-28T22:33:35.052Z_

## Thoughts

### 2026-01-28T22:33:51.307Z - Thought
Agent Meetings concept: Scheduled multi-agent deliberation (e.g., 08:00 daily) with defined roles (typescript-expert:Mentor:sonnet, testing-expert:User-Advocate:sonnet, gemini, codex, recall:Historian:haiku). Orchestrator manages agenda, turn limits, and captures output to memory think session. Core question: separate plugin or memory plugin feature?

### 2026-01-28T22:34:01.244Z - Thought
Scheduling feasibility: Linux/macOS have cron, Windows has Task Scheduler/schtasks. Cross-platform options: (1) node-cron library in persistent Node process, (2) Shell out to OS-specific schedulers via plugin hook. Challenge: Claude Code hooks are reactive (triggered by events), not proactive (scheduled). Need persistent orchestrator process or external scheduler that invokes Claude Code CLI/API.

### 2026-01-28T22:34:12.149Z - Thought
Separate plugin vs memory plugin feature: PRO-SEPARATE: (1) Clean separation of concerns - scheduling ≠ memory storage, (2) Users who don't want scheduled meetings don't pay the complexity cost, (3) Easier to maintain/test in isolation. CON-SEPARATE: (1) Duplication of memory think integration logic, (2) Users need multiple plugins for related functionality, (3) Harder to query meeting history from memory system.

### 2026-01-28T22:34:21.360Z - Thought
Orchestrator design challenges: (1) Turn management - enforce max turns per agent, prevent runaway conversations, (2) Agenda tracking - ensure topics are covered, skip completed items, (3) Agent coordination - round-robin? Priority queue? Dynamic based on relevance?, (4) Context passing - each agent needs previous conversation history, (5) Failure handling - what if gemini API times out mid-meeting?, (6) Output formatting - generate meeting minutes, action items, decisions.

### 2026-01-28T22:34:32.231Z - Thought
Technical implementation: Agent invocation from scheduled task is non-trivial. Options: (1) Direct API calls to Claude/Codex/Gemini APIs (requires API keys, billing), (2) Shell out to provider CLIs (claude --model sonnet, codex exec, gemini), (3) Use Task tool via programmatic interface? But Task tool is for agents within active Claude Code session. Key insight: scheduled meetings likely need to run OUTSIDE active Claude Code session, which means can't use hooks/Task tool directly.

### 2026-01-28T22:34:42.948Z - Counter-argument
Alternative simpler approach: Instead of autonomous scheduled meetings, create a '/meeting' skill that USER initiates manually. Skill spawns multiple Task agents in parallel with pre-defined roles, collects responses, and synthesizes into memory think session. No scheduling complexity, no persistent process, no cross-platform scheduler headaches. Trade-off: requires user to remember to run meetings, but arguably that's acceptable - developers already run standup/retros manually.

### 2026-01-28T22:40:56.784Z - Thought (model:sonnet style:Historian agent:recall [7a3a645e-c298-4a55-a6a5-640e4c67232a])
Scheduled agent meetings would create a new temporal layer in the memory system that requires careful integration design. Key considerations:

**Memory Type Classification:**
Meeting transcripts don't fit cleanly into existing types (decision/learning/artifact/gotcha). Proposal: introduce `meeting` memory type with specialized frontmatter:
- `meeting_date`: ISO timestamp
- `participants`: Array of agent roles (typescript-expert:Mentor:sonnet, etc.)
- `agenda`: Topics covered
- `outcomes`: Extracted action items, decisions, learnings

**Search & Retrieval Implications:**
Current memory search queries like "error handling" would need to traverse meeting transcripts alongside permanent memories. Challenge: meetings are verbose (multi-turn conversations) vs. permanent memories (distilled knowledge). Risk: noise overwhelming signal in search results.

**Proposed Dual-Layer Pattern:**
1. Store full meeting transcript as `temporary/meeting-YYYY-MM-DD.md` (auto-purge after 30 days?)
2. During/after meeting, extract key insights and promote to permanent memories with backlink: `source_meeting: meeting-YYYY-MM-DD`
3. Memory graph edges: meeting → decisions/learnings extracted from it

**Indexing Strategy:**
Meeting transcripts benefit from different indexing than permanent memories:
- Temporal queries: "meetings last week", "discussions about X in January"
- Participant queries: "meetings with typescript-expert", "what did gemini suggest about Y?"
- Agenda queries: "meetings covering authentication"

Suggest `memory list --type meeting --since "last week"` and `memory search "authentication" --scope meetings` to keep meeting retrieval separate but integrated.

**Graph Connectivity:**
Meetings risk becoming orphaned super-nodes with many outbound edges but no inbound. Health checks should flag meetings with zero promoted memories (indicates meeting produced no actionable insights → waste of tokens). Link suggestion between meeting transcripts and related permanent memories could help trace decision provenance: "This architecture decision emerged from the 2026-01-15 agent meeting."

**Historical Context Value:**
As recall agent, scheduled meetings address a gap: capturing *why* certain approaches were discussed, even if not chosen. Current memory system captures outcomes (decisions made) but loses deliberation context (alternatives considered, trade-offs debated). Meeting transcripts preserve that reasoning chain, making future recall more nuanced: "We chose approach A over B because of performance concerns raised by typescript-expert in the 2026-01-10 meeting."

**Recommendation:**
If implementing scheduled meetings, MUST include automated extraction/promotion pipeline to prevent context window bloat. Meeting transcripts are raw material; permanent memories are refined product. Don't query meetings directly except for historical audit; query the promoted memories with meeting provenance metadata.

### 2026-01-28T22:41:35.426Z - Thought (model:sonnet style:Product-Manager [f19dab1e-4b0d-4505-98e5-19ce4e550997])
Before jumping to implementation, let's validate whether developers actually want this.

**User problem validation:**
- What pain point does "scheduled agent meetings" solve?
- Existing tools (Slack standups, Linear updates, GitHub activity summaries) already provide async team context
- Is the problem "I forget to check my codebase status" or "I want AI to synthesise context for me"?

**Product-market fit concerns:**
The `/meeting` skill (user-initiated) versus scheduled autonomous meetings is a critical product decision:

**/meeting skill (manual trigger):**
- ✅ User controls when they need context (before starting work, end of day review)
- ✅ Zero notification fatigue - pull model, not push
- ✅ Fits existing developer workflow (deliberate context gathering)
- ✅ Trivial to implement - no scheduling infrastructure

**Scheduled autonomous meetings:**
- ❌ Push model - creates interruptions/notifications developers might ignore
- ❌ Assumes daily 08:00 is universally valuable (timezone issues, work patterns vary)
- ❌ Risk of "crying wolf" - if auto-generated summaries are low-signal, users tune out
- ❌ Massive implementation complexity for unvalidated value

**Competitive analysis:**
- GitHub Copilot Workspace doesn't auto-schedule agent reviews
- Cursor doesn't have scheduled AI check-ins
- Most AI dev tools are on-demand, not proactive push

**Validation questions before building:**
- Have any users requested scheduled meetings? Or are we building because it's technically interesting?
- Would developers actually read/act on automated daily summaries?
- What's the kill criteria if adoption is low after launch?

**Recommendation:**
Ship `/meeting` skill first (MVP that solves "I want multi-agent context synthesis"). Instrument usage. If developers run it daily at similar times, THEN consider auto-scheduling as a power-user feature. Don't build scheduling infrastructure for a hypothesis.

**Success metric to validate first:**
"After shipping `/meeting`, do 30%+ of active users run it at least 3x/week?" If no, scheduled meetings won't save a problem that doesn't exist.

### 2026-01-28T22:42:06.058Z - Counter-argument (model:sonnet style:Devils-Advocate [eec7637f-c6c9-4b78-b642-6c972b4ff77e])
The fundamental premise is flawed: we're building meeting infrastructure before proving meetings have value at all.

Nobody's asking for this. Check the issue tracker, support requests, user feedback - is there demand for scheduled agent discussions? Or is this a solution looking for a problem?

What's the actual user story? "As a developer, I want my agents to have meetings while I sleep so that..." what? So I come back to walls of agent debate I need to parse through? That's not a feature, that's inbox zero anxiety for AI conversations.

And the "historian" and "user advocate" roles - who decided these are useful? Have you tested whether multi-agent deliberation actually produces better decisions than a single well-prompted agent? Or are we just assuming more agents = better results?

The manual `/meeting` skill sidesteps the real question: do developers even want to orchestrate multi-agent discussions at all? Start with a simple experiment: create a one-off script that runs 3 agents on a test question, show it to real users, measure whether they find it valuable. If they don't use it more than once, scheduled meetings definitely aren't the answer.

Also consider the failure modes: what happens when a scheduled meeting deadlocks because two agents keep contradicting each other and hit turn limits? Who debugs that at 3am? The user wakes up to a broken automation they didn't witness and can't reproduce.

Before adding ANY of this - scheduling, orchestration, memory integration - prove that multi-agent deliberation is actually worth the cognitive overhead of reading agent transcripts. Otherwise you're building a complex feature that users will disable immediately.

### 2026-01-28T22:42:43.941Z - Thought (model:sonnet style:Architect [02352901-6c05-46cc-afb7-87292b3c330b])
The architectural challenge here is that we're conflating two distinct system concerns: **temporal orchestration** (scheduling work to happen at specific times) and **agent collaboration** (coordinating multiple LLM agents to deliberate on a topic).

**System boundary analysis:**

Current Claude Code architecture operates in **synchronous, user-initiated sessions**. All hooks (PreToolUse, PostToolUse, SessionStart) are reactive - they respond to user actions. Scheduled meetings would require **asynchronous, system-initiated sessions**, which is a fundamentally different execution model.

```
Current: User Action → Claude Code Process → Hooks → Tools → Response
Proposed: System Timer → ??? → Claude Code Process → Agent Orchestration
```

The `???` is the architectural gap. Claude Code has no daemon mode, no persistent process listening for scheduled events.

**Three architectural options:**

1. **External Scheduler + Headless Invocation**: OS scheduler (cron/Task Scheduler) invokes Claude Code CLI in headless mode with a special command like `claude code --meeting <session-id>`. Requires:
   - Headless execution mode (no user interaction)
   - Session state persistence (where to save meeting outputs?)
   - Authentication without user present (API keys in environment)

2. **Persistent Orchestrator Service**: Separate long-running Node.js process that schedules meetings and invokes Claude Code programmatically via its internal APIs. Requires:
   - Exposing Claude Code internal APIs for programmatic use
   - Service lifecycle management (start/stop/restart)
   - IPC mechanism (how does service talk to Claude Code sessions?)

3. **Simpler: Reminder-Based Meetings**: Instead of autonomous execution, SessionStart hook checks time and suggests: "It's 08:00 - time for your scheduled typescript-expert meeting. Run `/meeting typescript-review` to start." Requires:
   - No background processes
   - No scheduling complexity
   - User stays in control loop

**Dependency implications:**

Option 1 or 2 creates operational dependencies: monitoring, logging, failure alerts for background jobs. If scheduled meeting fails silently at 3am, how does user know? You need observability infrastructure.

Option 3 has zero new dependencies - it's a conditional prompt within existing session flow.

**My recommendation**: Start with Option 3 (reminder-based). Validate that multi-agent deliberations are actually useful first. If usage proves high value and users complain about manual triggering, THEN consider autonomous scheduling. Don't build scheduler infrastructure for unproven value.

The simpler architecture lets you focus on the hard part: designing effective agent collaboration patterns, turn management, and memory integration. Scheduling is a solved problem; agent orchestration quality is the actual innovation here.

### 2026-01-28T22:43:16.699Z - Thought (model:sonnet style:Pragmatist agent:typescript-expert [951592e9-dc5d-46d8-a67c-9b0402f02427])
From a TypeScript implementation standpoint, the architectural complexity here reveals a deeper design principle violation: **we're solving infrastructure before validating user workflow integration.**

The type system implications alone expose this:

```typescript
// What we're building
type ScheduledMeeting = {
  schedule: CronExpression;
  participants: AgentConfig[];
  orchestrator: OrchestratorProcess;
  stateManagement: PersistentState;
}

// What users actually need
type MeetingInvocation = {
  command: '/meeting';
  topic: string;
  participants?: AgentConfig[];
}
```

The scheduled version requires managing **mutable state across process boundaries** - TypeScript's strength is compile-time safety, but we'd be dealing with runtime scheduling, IPC, and cross-process synchronization. This pushes us into territories where TypeScript provides minimal guarantees (process crashes, network failures, partial writes to meeting state).

Counter-proposal grounded in TypeScript strengths: Build **meeting templates** first. A simple `~/.claude/meetings/daily-standup.yml` that defines participant roles and agenda structure. The `/meeting` skill reads this config, spawns agents via Task tool in the **current session**, and captures output to memory think. This is:

1. **Statically analyzable** - YAML schema validated at runtime via Zod/Yup
2. **Testable** - no mocking of schedulers or process managers
3. **Incrementally adoptable** - users can manually run `/meeting daily-standup` until they want scheduling

If usage metrics show developers run meetings daily at consistent times, **then** build the scheduler as a thin wrapper around proven meeting orchestration logic. The scheduler becomes a convenience layer, not a foundational architectural component.

Pragmatism dictates: ship the orchestration engine first as a manually-invoked skill. Let users validate the meeting format, agent interactions, and memory capture flow. Scheduling is a 50-line cron wrapper once the core works - but building it first means potentially throwing away complex infrastructure if the meeting concept itself doesn't resonate with users.

### 2026-01-28T22:43:57.546Z - Counter-argument (model:sonnet style:Risk-Assessor [91ffbaa9-faa9-44b0-8d66-7fb980d633f6])
The "scheduled meeting" framing fundamentally misunderstands how developers actually work with AI agents. Real risk isn't technical complexity—it's **incentive misalignment that leads to ignored output and wasted resources**.

**Critical failure mode: The "unread notifications" problem**
- Meeting runs at 08:00, generates 2000 tokens of analysis
- Developer arrives at 09:30, starts deep work on urgent bug
- Meeting output sits unread in memory system
- Next day: another meeting runs, more unread output accumulates
- Within a week: developer has 10,000+ tokens of stale meeting minutes they'll never read
- Result: Wasted API costs, cluttered memory, zero actual value delivered

**Why this fails where human meetings don't:**
- Human meetings have **forcing functions**: calendars block time, people notice if you don't show up, social pressure to engage
- Async meeting output has **no forcing function**: it's just another file/memory to ignore
- Developer workflow is interrupt-driven: they'll read what's relevant NOW, not what was scheduled yesterday
- Scheduled output becomes "notification debt" like unread emails—guilt-inducing but ultimately ignored

**The "agent meeting" concept suffers from solution looking for problem:**
- What specific decision needs daily multi-agent input? If it's that important, developer should trigger it themselves when they need it
- If output quality is high, why wouldn't developer want to control timing to get results when they're ready to act on them?
- If output quality is low, automating production of low-value content is worse than not having it at all

**The real user need is probably:**
- "Help me remember to check for test flakiness periodically" → Solution: `/memory search flaky tests` when they see failures, not scheduled reports
- "Surface technical debt before it becomes critical" → Solution: Hooks that flag issues during actual work (PreToolUse, PostToolUse), not scheduled audits
- "Get fresh perspective on stuck problems" → Solution: On-demand `/meeting` skill when DEVELOPER decides they need multi-agent input

**Honest cost-benefit:**
- Build cost: 40+ hours for scheduling, orchestration, cross-platform support, failure handling
- Maintenance burden: Ongoing support for "why didn't my meeting run?" / "how do I stop meetings?" / "meetings are costing me $50/month"
- Actual usage: 80% of users either (a) never enable it, (b) disable it after first week, or (c) ignore output
- Alternative: 4-hour `/meeting` skill implementation delivers 90% of value for 10% of cost

The scheduled meeting fantasy is seductive because it feels like having a team that works for you 24/7. Reality: it's a background process generating content you don't have time to consume, solving problems you didn't ask it to solve, at times you didn't choose.

### 2026-01-28T22:44:47.650Z - Thought (model:sonnet style:User-Advocate [aad78713-9a56-4782-987e-c88a3c495acc])
Users returning to their workspace shouldn't feel like they walked into the middle of a conversation they weren't invited to. That's exactly what autonomous scheduled meetings create.

The psychological contract with AI tools is crucial here. When I ask Claude to write code, review a PR, or search memory, there's a clear cause-and-effect: I requested action, the tool responded. That builds trust. But scheduled meetings break this contract—I didn't ask for anything, yet computational resources were consumed and decisions were potentially influenced without my awareness.

This manifests in concrete user pain:

**Notification fatigue**: "Agent meeting completed" notifications blend into background noise. Users learn to ignore them, just like they ignore automated build emails they didn't request. The signal-to-noise ratio drops to zero.

**Context loss**: I come back to find agents discussed refactoring approaches. But I was in the middle of debugging a production issue. The meeting conclusions are now irrelevant, but they're sitting in my memory system polluting future recalls with outdated priorities.

**Accountability gap**: If an agent meeting recommends a risky architectural change, who approved it? Not me—I was asleep. This creates a trust problem: I can't delegate architectural decisions to scheduled agents because I need to review their reasoning in the moment, ask follow-ups, inject constraints they don't know about.

**Recovery cost**: Reviewing a 30-agent turn discussion transcript to extract value takes longer than just having the discussion synchronously when I actually need it. Users end up skimming or ignoring the output entirely.

The alternative framing: scheduled meetings should be **prompts for user-initiated deliberation**, not autonomous discussions. At 08:00, notify me "Daily architecture review available" with a one-click action to spawn the multi-agent session RIGHT NOW while I watch. That preserves user control, ensures I'm present for critical decisions, and makes the output immediately actionable because I'm in the context.

### 2026-01-28T22:46:27.413Z - Alternative (model:haiku style:Simplifier [5d917cc2-dcbc-4686-8690-3dcdc850d25c])
The notification-based approach solves the control problem but introduces a different one: **notifications get dismissed, scheduled meetings never happen.**

Better alternative: **Lazy-loaded meeting snapshots.**

Instead of scheduling meetings to run, pre-compute what the agenda *would* be at specific times by analyzing the memory system's recent activity. At 08:00, the system doesn't execute agents—it just generates a summary document showing:
- Issues/PRs touched in the last 24h
- Recent memories added (decisions, gotchas, learnings)
- Which agents would be relevant to current work
- A single button: "Start meeting with [Agent1, Agent2] on [Topics]"

This is a **passive notification** not an active orchestrator. The user gets context without surprise autonomous work. They can choose to run the meeting, or skip it if they're in a different context.

Technical win: No scheduling infrastructure, no persistent process, no turn management complexity. Just a cron job that generates a static HTML report. Framework already handles the memory queries.

The real insight: developers don't want autonomous meetings. They want **right-time suggestions**—"hey, you touched three authentication files this week, might be worth getting typescript-expert and security-expert in a room to review." That's not a meeting infrastructure problem, it's a **context awareness** problem. Solve that, and scheduled meetings become an obvious optional feature layered on top, not a foundational architectural choice.

### 2026-01-28T22:47:08.943Z - Thought (model:sonnet style:Pragmatist agent:documentation-accuracy-expert [67d5133d-0717-4b18-be0f-a5d430c528e7])
The documentation burden argument reveals something deeper: **feature learnability inversely correlates with adoption**. Scheduled meetings require upfront configuration investment before users understand the value. That's backwards.

Better onboarding path: `/meeting` command works immediately with sensible defaults (typescript-expert, testing-expert, recall as participants). Users experience value in first invocation. *Then* they care about customization. Progressive disclosure beats comprehensive configuration.

But there's a subtler issue: **scheduled meetings optimize for consistency at the expense of relevance**. Daily 08:00 meeting happens whether you shipped code yesterday or not. Manual invocation self-selects for "I actually need input right now" moments. The latter has higher signal-to-noise ratio.

Testing story also matters. How do you test scheduled meetings without actually waiting for cron? Mock time? Mock scheduler? Meanwhile `/meeting` skill is just a command - test it like any other skill. Testability predicts maintainability.

The real decision isn't scheduled vs manual. It's **proactive automation vs on-demand tooling**. Claude Code's strength is reactive assistance - you code, it helps. Scheduled meetings break that model by adding autonomous behavior. That's not inherently wrong, but it requires justifying why this feature deserves to violate the core interaction pattern when manual `/meeting` achieves 90% of the value with 10% of the complexity.

### 2026-01-28T22:49:20.753Z - Thought (model:sonnet style:Risk-Assessor agent:code-quality-expert [29c970e7-5d33-42a5-970b-8d914c297c9c])
**Risk assessment for scheduled agent meetings:**

**High severity:**

- **Runaway cost** - Agent meeting runs daily at 08:00 while developer sleeps → agents hit 100-turn debate → £50 API bill before anyone notices. Likelihood: High without hard limits and kill switches.

- **State corruption during active work** - Scheduled meeting fires whilst user is mid-implementation → memory think session modified concurrently → merge conflict in deliberation state → either user's work or meeting output corrupted. Likelihood: Medium for daily schedules.

- **Silent failures accumulate** - Meeting fails to run due to API timeout/network issue → no alert → developer assumes agents are monitoring codebase → week passes → backlog of unreviewed changes. Likelihood: High without robust monitoring.

**Medium severity:**

- **Zombie processes** - Orchestrator crashes mid-meeting → child agent processes orphaned → continue consuming resources until reboot. Likelihood: Medium on long-running meetings.

- **API key exposure** - Scheduled task needs Anthropic/OpenAI keys → stored in cron environment or config file → file permissions wrong → keys leaked. Likelihood: Medium, especially on shared dev machines.

- **Clock skew chaos** - Developer's laptop suspended overnight → system clock off by hours → cron triggers 6 missed meetings simultaneously → resource exhaustion. Likelihood: Low on desktop, high on laptops.

**Operational risks:**

- **Reproducibility nightmare** - "Agent meeting decided X yesterday" → user disagrees → wants to re-run → but yesterday's codebase state, git ref, dependencies no longer exactly reproducible → different conclusion.

- **Notification fatigue** - Meeting generates 10 notifications daily → developer ignores all → actual urgent alerts (security vulnerability found) also ignored. Classic boy-who-cried-wolf.

- **Version skew** - Plugin updated whilst scheduled meeting still using old version → new meeting format incompatible with old memory think structure → data loss.

**Edge cases to test:**

- System hibernates during scheduled meeting
- Disk full when writing meeting minutes
- Meeting scheduled during plugin upgrade
- Network down (API calls fail but file system writes succeed)
- Multiple plugin instances on same machine (different projects)
- Developer in different timezone than schedule configured for

**Monitoring gaps:**

- How do you know a meeting ran but produced garbage output?
- How do you know the orchestrator is alive between meetings?
- How do you detect an agent stuck in infinite loop during meeting?
- How do you track API spend per meeting over time?

**Blast radius:**

If agent meeting logic has a bug that writes corrupted data, it affects **all** memory think sessions going forward because meeting output feeds into future deliberations. Single bad meeting poisons the well.

### 2026-01-28T22:49:58.483Z - Alternative (model:sonnet style:Architect [4cd56eae-6ddc-4b80-be5a-c3808c50d2e4])
The control tension reveals a deeper systems design opportunity: **event-driven triggers instead of time-based scheduling**.

Rather than fighting the "when should this run?" problem, reconceive agent meetings as **response mechanisms to system events**:

```
Trigger Events:
- Pre-push hook: "Should we ship this?"
- Post-100-commits: "Architecture drift check"
- After test failure spike: "Quality retro"
- Memory health degradation: "Knowledge garden review"
- Dependency updates available: "Risk assessment meeting"
```

**Architectural advantages:**

1. **Context-rich invocation**: Meeting knows WHY it's running. Pre-push meeting has access to diff, changed files, test results. This context makes agent deliberation immediately actionable.

2. **Natural cadence**: Meetings happen when relevant work occurs, not on artificial schedules. Active projects get more meetings, dormant ones get none. Self-regulating.

3. **Fits existing infrastructure**: Hook system already exists. No persistent processes, no schedulers, no cross-platform compatibility matrix. Just PreToolUse/PostToolUse hooks that spawn Task agents.

4. **User control preserved**: Hooks can be disabled per-project. Users still choose when to push/commit/test, which transitively controls when meetings trigger.

5. **Immediate value feedback**: Meeting output appears exactly when the user can act on it. Pre-push meeting that says "don't ship this, auth bug detected" prevents the push. Tighter feedback loop than daily standup that surfaces issues hours after they're created.

**Dependency boundaries:**

```
[Git Operations] → [Hook Trigger] → [Meeting Orchestrator] → [Multiple Agents via Task] → [Memory Think Session]
```

This inverts the control flow. Instead of "memory plugin schedules meetings", it becomes "project lifecycle events request multi-agent analysis". Memory plugin provides the meeting infrastructure (orchestration, synthesis), but triggering is decentralised to whatever hooks users configure.

**Evolution path**: Start with ONE hook-triggered meeting type (e.g., pre-push quality gate). Validate whether multi-agent deliberation provides value in that specific context. If yes, expand trigger library. If no, kill the feature before building scheduling infrastructure.

Trade-off: Less predictable than "daily 08:00 standup". But prediction isn't the actual goal—**relevant insight at decision points** is. Event-driven meetings optimise for relevance over regularity.

### 2026-01-28T22:50:27.474Z - Thought (model:sonnet style:Pragmatist agent:test-quality-expert [a4b49ae9-5441-4c80-93ad-c3ed96ea6870])
The test maintenance burden isn't just high—it's a sustainability red flag. Time-based tests are notorious for CI flakiness, and here's the pragmatic reality: **untested code ships, or tests become so expensive that features get cut**.

Let's quantify the test burden:

**Scheduled meetings require:**
- Mocking `cron`, `node-cron`, Task Scheduler, `schtasks`
- Time-travel test utilities (advancing system clock, handling timezone edge cases)
- Multi-process cleanup (orphaned scheduler processes failing subsequent tests)
- Async coordination (waiting for scheduled task to fire, agent responses to arrive, memory writes to complete)
- Platform-specific CI matrices (Linux/macOS/Windows with different scheduler implementations)

That's **easily 40%+ of total test code** dedicated to scheduling infrastructure, not meeting orchestration logic.

**Manual `/meeting` command requires:**
- Standard CLI argument parsing tests
- Agent invocation mocks (Task tool, API calls)
- Response aggregation unit tests
- Memory integration tests (already exist for `memory think`)

That's **10-15% test overhead**, mostly reusing existing test infrastructure.

The kicker: **scheduled meetings have higher ongoing maintenance cost**. Every Node.js upgrade, OS scheduler change, or CI platform migration risks breaking time-based tests. The `/meeting` command is stateless—invoke, run, done. No persistent processes, no scheduler synchronisation, no flaky timing assertions.

From a shipping perspective: the `/meeting` command can be implemented, tested, and documented in a single sprint. Scheduled meetings need persistent process management, cross-platform scheduler integration, robust failure recovery, and extensive testing—that's multi-sprint work before any user sees value.

Ship the manual command. If users run it daily for three months and request automation, *then* consider scheduling. Don't build infrastructure for a workflow that doesn't exist yet.

### 2026-01-30T17:15:10.637Z - Conclusion
Agent Meetings concept validated architectural concerns but revealed bigger opportunity: autonomous agent orchestration. Consensus (10 agents): prove value with manual /meeting skill before building scheduling. Key insight: scheduling is easy; orchestration with memory persistence is the innovation. Decision: defer Agent Meetings to new 'Accord' plugin with broader scope - persistent working agents, meeting orchestration, dashboard UI, and local memory per agent. Agent Meetings becomes subset of Accord's capabilities.
