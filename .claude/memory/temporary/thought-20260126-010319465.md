---
type: breadcrumb
title: "Think: Should v1.2.0 prioritise test isolation fixes or new features?"
topic: Should v1.2.0 prioritise test isolation fixes or new features?
status: active
created: "2026-01-26T01:03:19.468Z"
updated: "2026-01-26T01:05:01.463Z"
tags:
  - think
  - active
scope: project
---

# Should v1.2.0 prioritise test isolation fixes or new features?

_Thinking document created 2026-01-26T01:03:19.468Z_

## Thoughts

### 2026-01-26T01:03:26.264Z - Thought
357 test failures from mock pollution is embarrassing tech debt that undermines confidence in the test suite

### 2026-01-26T01:03:49.842Z - Thought (model:haiku [139f0bc7-2ab1-4e73-a447-e6b87c928db8])
Test isolation fixes aren't invisible—they're the difference between a test suite that actually tells you what's broken versus one that lies to you through cascading failures. When you ship v1.2.0 with 357 phantom failures, every user who runs tests locally will assume your plugin is broken. You'll spend more time triaging false positives than building features anyway.

The real cost of shipping with mock pollution is that new features become suspect—did I break something, or is it just the test suite being unreliable again? That uncertainty kills momentum faster than delayed releases do. Plus, you're training yourself and future maintainers to ignore test failures, which is a habit that's remarkably difficult to break once it's ingrained.

New features on a foundation of broken tests are just technical debt wearing a feature hat. You'll rewrite them when you finally fix the tests and discover they don't actually work. Fix the isolation issues first, then build the features on solid ground where you can trust what the tests are telling you.

### 2026-01-26T01:04:24.426Z - Counter-argument (model:haiku [90ba0d24-1bfd-4656-ad9d-987d73d32379])
Users won't see test failures locally because most won't run the test suite at all—they'll install the plugin and either it works or it doesn't. What they *will* see is features that solve their problems. A v1.2.0 with three new capabilities and a known test isolation issue is objectively more valuable than a v1.2.0 that fixes internal plumbing nobody interacts with.

The 357 failures are a CI/CD problem, not a user problem. If the test suite is so broken that it's unreliable, that's a development velocity issue, sure—but the fix for that is to either quarantine the failing tests or rewrite the test infrastructure, not delay features. Meanwhile, your backlog sits there getting older and your users keep asking for the same things on GitHub.

Test isolation is invisible labour. Features are capital. You can ship v1.2.0 with features *and* create a separate v1.2.1 hotfix sprint for test isolation if it genuinely becomes a problem. But right now you're choosing between "users get something new" and "developers feel better about themselves." Only one of those actually moves the product forward.

### 2026-01-26T01:05:01.463Z - Thought (model:haiku [5c5454ee-17d0-4554-950d-205cfa3078b3])
The counter-argument conflates "users won't notice" with "it doesn't matter," which is classic short-term thinking dressed up as pragmatism. Here's why test isolation fixes actually *do* matter for v1.2.0:

**Contributors and maintainers are users too.** Anyone who forks your repo, opens a PR, or tries to understand why their contribution broke something will immediately hit 357 cascading failures. You've just created a hostile environment for collaboration. New contributors will assume the codebase is a mess and either won't bother contributing or will waste hours debugging phantom failures that aren't their fault.

**The test suite becomes a liability, not an asset.** When failures don't correlate with actual problems, developers stop trusting test feedback. They run tests locally, see red, assume it's the usual pollution nonsense, and ship code without proper validation. You've accidentally incentivised ignoring test results—which is the opposite of what you want from a test suite.

**Test isolation bugs compound exponentially.** Every new feature added to a codebase with mock pollution gets tangled in the same broken patterns. By v1.3.0, you won't just have 357 failures—you'll have 600. The debt becomes actively harder to repay because it's spread across more test files and more mocking patterns.

The real question isn't "do end users care?" It's "do *you* care about shipping code you're confident in?" A v1.2.0 with three features built on top of a test suite you know is broken is three features you can't actually guarantee work correctly. The features aren't more valuable if the confidence in them is illusory.
