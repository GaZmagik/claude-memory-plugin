---
type: breadcrumb
title: "Think: Output styles for memory assessment agents"
topic: Output styles for memory assessment agents
status: concluded
created: "2026-01-24T19:54:20.350Z"
updated: "2026-01-24T20:03:50.375Z"
tags:
  - think
  - concluded
scope: project
conclusion: "v1.1.0 will focus on three features: (1) Enhanced hint visibility via stderr hints, interactive prompts for complex thoughts, and better --help examples; (2) --auto flag using ollama for intelligent agent/style/model selection with transparency and graceful degradation; (3) Enhanced memory injection configurable via memory.local.md to inject gotchas/decisions/learnings based on relevance. Assessment profiles deferred to v1.2.0. Architecture follows 'separate concerns, combine via presets' pattern."
promotedTo: decision-output-styles-for-memory-assessment-agents
---

# Output styles for memory assessment agents

_Thinking document created 2026-01-24T19:54:20.350Z_

## Thoughts

### 2026-01-24T19:54:28.547Z - Thought
Current plugin has styles like 'sardonic' for personality. Memory assessment needs different kinds of styles - ones that shape HOW the agent analyzes memories, not just tone. Think: analytical depth, skepticism level, focus areas.

### 2026-01-24T19:54:35.422Z - Thought
Assessment style categories to consider:
1. **Curator** - Focuses on graph health, orphaned nodes, missing links, tag consistency
2. **Skeptic** - Questions validity, checks if memories are still accurate, flags outdated info
3. **Archivist** - Identifies duplicates, suggests consolidation, enforces naming conventions
4. **Archaeologist** - Traces memory evolution, finds patterns across time, identifies knowledge gaps

### 2026-01-24T19:54:44.032Z - Counter-argument
But wait - are these really 'output styles' or are they more like 'assessment modes' or 'agent personas'? Output styles in Claude Code shape response formatting and tone. What we're describing is more like specialized behavior patterns. Maybe this needs a different abstraction entirely - perhaps 'assessment profiles' that combine: style + focus areas + validation rules + recommended model.

### 2026-01-24T19:54:51.738Z - Alternative
Assessment Profile structure could look like:
```yaml
name: memory-skeptic
description: Validates memory accuracy and relevance
style: analytical  # Output tone
focus:
  - accuracy-validation
  - staleness-detection  
  - contradiction-finding
model: haiku  # Fast, cheap for bulk assessment
prompts:
  system: 'You are a skeptical reviewer...'
  per-memory: 'Assess this memory for accuracy...'
actions:
  - flag-for-review
  - suggest-update
  - mark-stale
```

### 2026-01-24T19:55:00.675Z - Thought
How would this integrate with existing plugin? Current flow:
- `memory think` creates deliberation sessions for the USER's thoughts
- New feature: `memory assess` would run AGENT assessments on stored memories

The key difference: think = human deliberation captured; assess = automated agent analysis

### 2026-01-24T19:55:50.980Z - Thought (model:haiku style:Architect [4fdb8c48-da7a-48af-a34c-ede66c34ab3e])
Excellent. The deliberation has concluded and been promoted to a permanent decision in memory. Here's what we accomplished:

## Deliberation Complete

**Topic**: Output styles for memory assessment agents

**Perspectives gathered**: 4 thoughts
- 1 architectural composition analysis (decoupling concerns)
- 1 pragmatist counter-argument (user experience vs purity)
- 1 progressive enhancement branch (preset-based with composition underneath)
- 1 risk assessment (serialisation and versioning challenges)

**Outcome**: Promoted to `decision`

## Key Architectural Insight

The deliberation resolved a fundamental tension: **internal composition vs external simplicity**. Rather than choosing one, the recommendation is a **layered approach**:

**Layer 1 (User-facing)**: Curated preset profiles
- Names like "sceptical", "thorough", "quick" that bundle validator + analyser + formatter combinations
- Reduces cognitive load for typical users

**Layer 2 (Internal)**: Composable components
- Validators, analysers, and formatters are independently injectable
- Enables clean testing and future extensibility

**Layer 3 (Advanced)**: Custom profiles via JSON
- Power users can compose their own combinations
- Semantic versioning ensures component compatibility

**Critical caveat**: Serialisation and backwards compatibility must be designed upfront. This isn't something you bolt on laterâ€”it affects the entire profile representation strategy.

This pattern mirrors how good systems evolve: simple defaults that don't obscure the composable architecture underneath. You ship for the 80%, but the structure supports the 20%.

The decision is now stored in your project memory and can inform implementation decisions for the plugin.

### 2026-01-24T19:56:01.043Z - Thought
User idea: Generalize 'gotcha injection' â†’ 'memory injection'. Currently hooks inject gotchas into context. But what if memory.local.md could configure which memory TYPES get injected?

```yaml
# .claude/memory.local.md frontmatter
inject:
  gotchas: true        # default behavior
  learnings: false     # opt-in
  decisions: true      # project decisions always relevant
  artifacts: false     # usually too verbose
  scope: project       # or 'global' or 'both'
  relevance: 0.7       # semantic similarity threshold
```

This transforms the feature from 'gotcha prevention' to 'contextual memory augmentation' - the agent gets relevant project knowledge automatically surfaced.

### 2026-01-24T19:56:09.590Z - Counter-argument
Context bloat risk: Gotchas are high-signal, low-volume. But learnings/decisions could flood context. Need careful defaults:
- Gotchas: inject by default (they prevent mistakes)
- Decisions: inject only when HIGHLY relevant (threshold 0.85+)
- Learnings: opt-in only (too many, too verbose)
- Artifacts: never auto-inject (reference on demand)

Also: injection timing matters. SessionStart = bulk context. PreToolUse = surgical relevance. Different memory types may need different injection points.

### 2026-01-24T19:56:19.375Z - Alternative
Connecting the dots: Output styles + Assessment profiles + Memory injection are three related features:

1. **Output Styles** (existing) - How Claude responds (tone, format)
2. **Assessment Profiles** (new) - How an agent evaluates memories
3. **Memory Injection** (enhanced) - What memories get surfaced to context

These could share config in memory.local.md:
```yaml
# Assessment profile for this project
assessment:
  profile: skeptic
  schedule: on-commit  # or manual, daily
  
# Injection settings
inject:
  types: [gotchas, decisions]
  threshold: 0.75
  
# Output style override for memory commands
style: analytical
```

One config file, three capabilities.

### 2026-01-24T19:56:43.754Z - Thought (model:haiku style:Devils-Advocate [16bcbdf0-581d-4f53-9ae2-6275a1e841bc])
The unified config approach creates a false sense of elegance. You're conflating three distinct concerns: how the agent presents findings (styles), how it conducts analysis (profiles), and what context it receives (injection). Collapsing these into one file means:

**Coupling risk**: Changing assessment depth now requires touching style config. A user wants "more skeptical analysis" and accidentally breaks their tone preferences.

**Discoverability problem**: New users won't know whether to tweak assessment profiles or memory injection to fix what they perceive as a single problem. The config becomes a black box.

**Versioning nightmare**: If assessment profiles evolve (you add a new validation rule), it affects the injection defaults. Rollback becomes unclear - was the change breaking because of the profile or the injection config?

However, there's a real pattern here worth capturing: certain *combinations* genuinely do work together. A "ruthless-auditor" assessment profile probably wants high-skepticism style + aggressive gotcha injection + learnings filtered for contradictions. A "synthesis-builder" profile wants collaborative tone + minimal gotcha injection + decision-heavy memory.

Better solution: Keep them separate but create **preset combinations**. A `.claude/memory-assessment-presets.json` that says "auditor includes: style:sardonic + profile:aggressive + injection:gotchas-only". User picks a preset, or mixes-and-matches if they know what they're doing.

This way you get composability without coupling, discoverability without cognitive overload, and users can understand what each lever does independently before combining them.

### 2026-01-24T19:56:53.864Z - Thought
Devil's Advocate makes a good point: separate concerns, combine via presets. So the architecture becomes:

**Separate configs:**
- `styles/` - Output formatting (existing)
- `profiles/` - Assessment behaviors (new)
- `memory.local.md` - Injection settings (enhanced)

**Preset combinations:**
```yaml
# .claude/memory-presets/auditor.yaml
name: auditor
description: Aggressive memory quality enforcement
components:
  style: analytical
  profile: skeptic
  inject:
    types: [gotchas]
    threshold: 0.6
```

User runs: `memory assess --preset auditor` or sets default in memory.local.md

### 2026-01-24T19:57:07.276Z - Thought
Meta-insight: The `memory think --call claude --agent X --style Y` pattern is powerful but not discoverable. A hook could inject a reminder when plain `memory think add` is used without --call claude:

'Tip: Use --call claude for AI-assisted deliberation. Add --style <name> for perspective or --agent <name> for domain expertise.'

This teaches the workflow while using it - progressive disclosure of capabilities.

### 2026-01-24T19:57:27.575Z - Thought
Summary of what emerged from this deliberation:

**Three distinct features identified:**
1. **Assessment Profiles** - How agents evaluate memories (skeptic, curator, archivist, archaeologist)
2. **Memory Injection** (enhanced) - Configurable injection of gotchas/learnings/decisions/artifacts  
3. **Preset Combinations** - Bundles of style + profile + injection for common workflows

**Key architectural decision:** Keep concerns separate, combine via presets. Don't unify configs.

**Implementation order suggestion:**
1. First: Enhanced memory injection (low effort, high value) - just expand existing gotcha hooks
2. Second: Assessment profiles (medium effort) - new `memory assess` command
3. Third: Presets (ties it together) - convenience layer on top

### 2026-01-24T20:01:55.820Z - Thought (model:haiku agent:nodejs-expert [a962dedd-0484-4313-a3cf-58c8e1a4dba5])
The discoverability problem you've identified is real, but injecting hints on every `memory think add` creates noise fatigue. Users will tune it out within three uses.

Consider instead: contextual hints triggered by *absence patterns*. If a user runs `memory think add` followed by `memory think add` again without `--call claude` between them, that's when you hint. First solo addition? Let them explore. But repeated solo additions suggest they don't know about collaborative deliberation.

Better yet: make the hint *optional and persistent*. Store in `.claude/memory/.local.md` a `hints_shown` list. Only display hints the user hasn't dismissed yet. They can do `memory hint disable --pattern "collaborative"` or `memory hint enable --all`.

This avoids three problems:
- Notification fatigue (users only see each hint once by default)
- False positives (new users get one or two adds before being prompted)
- Inflexibility (power users can mute specific hint categories)

For display: stderr with color is correctâ€”it separates signal (stdout: your results) from meta-information (stderr: guidance). But make the color optional via `--no-hints` or environment variable `CLAUDE_MEMORY_QUIET=1`.

The bigger architectural question though: is this hint system worth the complexity? Or should the plugin just surface this in onboarding docs or a `memory help think` command? Hints in CLI tools work best when they're truly surprising insights ("Did you know you can chain --call commands?") not workflow guidance that belongs in documentation.

What's the actual user pain point you're trying to solveâ€”discoverability, or users genuinely not understanding the collaborative potential?

### 2026-01-24T20:02:29.616Z - Thought (model:haiku style:Pragmatist [2384fb5c-65a4-4216-96dc-182f8a2eb6a7])
The real problem isn't discoverabilityâ€”it's that users don't know when to use multi-agent thinking. They're doing `memory think add` because that's the default mental model. Hints won't change that.

Instead: make the tool proactive. When `memory think add` detects a complex topic (multiple branches, counter-arguments, or length >300 chars), automatically suggest the --call pattern. Not as a warning, as a question: "This looks like it could benefit from [specialist perspective]. Want me to add the nodejs-expert's take?" Users opt-in, no fatigue.

Better still: ship a preset system. `memory think add --preset "architectural-decision"` pulls in suggested agents, styles, and branching structure. Presets are discoverable in help, composable, and don't require learning the whole CLI. You get 80% adoption without the complexity.

The unified config was overengineered. Three separate systems (styles, profiles, injection) that _compose_ via presets solves it without the false elegance problem.

### 2026-01-24T20:02:32.007Z - Alternative
NEW FEATURE: `--auto` flag for intelligent agent/style/model selection

Concept: Instead of manually choosing --call claude --agent X --style Y --model Z, user runs:
```bash
memory think add 'Should we use Redis or SQLite for caching?' --auto
```

Behind the scenes:
1. Send thought text to local ollama (fast, cheap, private)
2. Ollama analyzes and returns: {agent: 'typescript-expert', style: 'Architect', model: 'sonnet'}
3. Plugin then calls claude with those params

Benefits:
- Removes friction from collaborative deliberation
- Users don't need to know which agents/styles exist
- Ollama selection can learn from past deliberations (future enhancement)

### 2026-01-24T20:03:03.762Z - Thought (model:haiku style:Devils-Advocate [b8060c99-9fdc-499c-8f11-ee769f6eeaac])
The `--auto` flag creates a hidden decision layer that users can't inspect or override mid-session. When it picks the wrong agent for a deliberationâ€”and it will, statisticallyâ€”users won't understand why their thinking went sideways. They'll blame the feature, or worse, blame themselves for not understanding what "auto" actually does.

What happens when `--auto` conflicts with explicit flags? If a user writes `memory think --auto --style sardonic`, does sardonic win or does auto override it? That ambiguity will create frustration.

And the ollama dependencyâ€”you're outsourcing agent selection to a local model that might not understand your plugin's agent taxonomy. It could suggest an agent that doesn't exist in this workspace, or pick agents that are overkill for simple thoughts. You've traded one discoverability problem for a reliability problem.

Plus, training data staleness: ollama's understanding of your agents is only as fresh as the last time you updated the model context. As new agents get added to the plugin, auto-selection becomes progressively worse without explicit retraining.

The pragmatist's point about mental models is strongerâ€”users need to *know* when multi-agent thinking is valuable, not have it selected for them. A better solution might be: keep `--auto` as a future optimization, but first solve the discoverability problem with smarter defaults and better documentation of when each agent combination makes sense. Let users make intentional choices before automating them away.

### 2026-01-24T20:03:06.444Z - Thought
Implementation detail for --auto: The ollama selection prompt could be:

```
Given this thought for deliberation:
'{thought_text}'

And this context from the current deliberation:
'{topic}: {thought_count} thoughts so far'

Select the most appropriate:
- agent: one of [typescript-expert, nodejs-expert, python-expert, security-code-expert, etc.]
- style: one of [Architect, Devils-Advocate, Pragmatist, Optimist, Skeptic]
- model: one of [haiku, sonnet, opus] based on complexity

Return JSON: {agent, style, model, reasoning}
```

Use a small/fast ollama model (llama3.2:3b or similar) - this is just routing, doesn't need heavy reasoning.

### 2026-01-24T20:03:24.661Z - Thought
Addressing Devil's Advocate concerns about --auto:

1. **Transparency**: Show the selection before executing:
   `--auto picked: agent=typescript-expert style=Architect model=haiku`
   `Proceed? [Y/n/edit]`
   
2. **Override precedence**: Explicit flags always win. --auto only fills in gaps.
   `--auto --style sardonic` â†’ auto picks agent/model, sardonic style used
   
3. **Agent taxonomy**: Embed current agent list in the prompt dynamically - read from plugin config at runtime, not hardcoded.

4. **Fallback**: If ollama unavailable or returns invalid selection, gracefully degrade to sensible defaults (haiku, no agent, no style).

This makes --auto an 'assisted selection' not 'automatic takeover'.

### 2026-01-24T20:03:27.823Z - Alternative
HINT VISIBILITY: Concrete implementation options

**Option A: stderr colored hint (first N uses)**
```
ðŸ’¡ Tip: Add --call claude for AI perspectives
   Example: memory think add 'topic' --call claude --style Architect
```
Show 3 times, then suppress. Reset with `memory config hints.reset`

**Option B: Interactive prompt on complex thoughts**
When thought >200 chars or contains '?' - prompt:
`This looks like a question. Get an AI perspective? [y/N]`

**Option C: Example in help output**
`memory think add --help` shows:
```
EXAMPLES:
  memory think add 'My thought'                    # Solo thinking
  memory think add 'Topic' --call claude           # Get AI perspective  
  memory think add 'Topic' --auto                  # Auto-select best AI
```

**Recommendation**: Do all three. A + C are low-effort, B is the 'proactive' approach the Pragmatist suggested.

### 2026-01-24T20:03:41.656Z - Thought
v1.1.0 FEATURE SUMMARY for speckit explore:

**Feature 1: Enhanced Hint Visibility**
- stderr colored hints (first 3 uses per command)
- Interactive prompt for complex thoughts (>200 chars or questions)
- Better --help examples with --call claude patterns
- Low effort, high discoverability impact

**Feature 2: --auto Flag**
- Ollama-powered agent/style/model selection
- Transparent: shows selection, asks for confirmation
- Graceful degradation if ollama unavailable
- Explicit flags override auto-selection
- Medium effort, removes friction from collaborative deliberation

**Feature 3: Enhanced Memory Injection** (from earlier deliberation)
- Configurable via memory.local.md
- Inject gotchas (default), decisions (opt-in), learnings (opt-in)
- Relevance threshold and scope settings
- Low-medium effort, transforms 'gotcha prevention' to 'contextual augmentation'

**Feature 4: Assessment Profiles** (future - v1.2.0?)
- memory assess command with skeptic/curator/archivist profiles
- Separate from but composable with output styles
- Higher effort, defer to later release

Ready for: `/speckit:explore` on features 1-3 for v1.1.0

### 2026-01-24T20:03:50.374Z - Conclusion
v1.1.0 will focus on three features: (1) Enhanced hint visibility via stderr hints, interactive prompts for complex thoughts, and better --help examples; (2) --auto flag using ollama for intelligent agent/style/model selection with transparency and graceful degradation; (3) Enhanced memory injection configurable via memory.local.md to inject gotchas/decisions/learnings based on relevance. Assessment profiles deferred to v1.2.0. Architecture follows 'separate concerns, combine via presets' pattern.
