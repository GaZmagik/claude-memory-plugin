---
type: breadcrumb
title: "Think: v1.1.0 Cross-Provider Agent Calling (Codex & Gemini)"
topic: v1.1.0 Cross-Provider Agent Calling (Codex & Gemini)
status: concluded
created: "2026-01-24T20:16:44.390Z"
updated: "2026-01-24T20:40:26.847Z"
tags:
  - think
  - concluded
scope: project
conclusion: "v1.1.0 will add basic cross-provider support: --call codex and --call gemini with --model as the only portable parameter. Codex also supports --oss flag for local models (gpt:oss-20b/120b). CLIs must be installed or command fails gracefully. Full feature set (--style via prompt injection, provider-specific params like --profile/--extensions/--agent) deferred to v1.2.0. This keeps v1.1.0 scope manageable while proving the multi-provider concept."
promotedTo: decision-v1-1-0-cross-provider-agent-calling-codex-gemini
---

# v1.1.0 Cross-Provider Agent Calling (Codex & Gemini)

_Thinking document created 2026-01-24T20:16:44.390Z_

## Thoughts

### 2026-01-24T20:16:53.046Z - Thought
CLI PARAMETER EXPLORATION RESULTS

Investigated `codex --help` and `gemini --help` to understand what parameters are portable across providers.

**Universal Parameters:**
- Model selection: All three support this
  - Claude: `--model haiku/sonnet/opus`
  - Codex: `-m, --model <MODEL>` (gpt-5-codex, o3, etc.)
  - Gemini: `-m, --model <string>` (gemini-2.5-pro, etc.)

**Claude-Only Parameters:**
- `--agent <name>`: Specialized subagent selection (typescript-expert, nodejs-expert, etc.)
- `--style <name>`: Output personality (Architect, Devils-Advocate, Sardonic, etc.)

**Codex-Specific:**
- `-p, --profile <name>`: Config presets from ~/.codex/config.toml
- `--search`: Enable web search capability
- `--oss` / `--local-provider`: Use local models (ollama, lmstudio)

**Gemini-Specific:**
- `-e, --extensions <list>`: Modular plugin capabilities
- `-o, --output-format`: text/json/stream-json
- `--allowed-tools`: Pre-approve specific tools

### 2026-01-24T20:17:05.024Z - Thought
DESIGN IMPLICATIONS FOR MEMORY THINK --call

**Option A: Minimal - Model Only**
Support only `--model` for non-Claude providers:
```bash
memory think add 'Topic' --call codex --model gpt-5-codex
memory think add 'Topic' --call gemini --model gemini-2.5-pro
```
- Pros: Simple, reliable, no parameter translation needed
- Cons: Loses the rich --agent/--style capabilities that make Claude calls valuable

**Option B: Provider-Specific Parameters**
Each provider gets its own supported params:
```bash
# Claude (full support)
memory think add 'Topic' --call claude --agent typescript-expert --style Architect --model sonnet

# Codex (profile-based)
memory think add 'Topic' --call codex --profile research --model o3

# Gemini (extension-based)  
memory think add 'Topic' --call gemini --extensions code-review --model gemini-2.5-pro
```
- Pros: Leverages each provider's native capabilities
- Cons: Complex implementation, user needs to know provider-specific options

**Option C: Abstraction Layer**
Map generic params to provider equivalents:
```bash
memory think add 'Topic' --call codex --persona skeptic  # Maps to codex profile
memory think add 'Topic' --call gemini --persona skeptic  # Maps to gemini extension or system prompt
```
- Pros: Unified UX across providers
- Cons: Leaky abstraction, may not map cleanly, maintenance burden

### 2026-01-24T20:17:17.360Z - Counter-argument
The abstraction layer (Option C) is seductive but dangerous. Codex profiles and Gemini extensions are fundamentally different concepts from Claude's agents/styles:

- **Claude agents** are specialized subagent prompts with curated tool access
- **Codex profiles** are config file presets (model, sandbox settings, approval policies)
- **Gemini extensions** are plugins that add capabilities (like MCP servers)

Mapping 'skeptic persona' to a Codex profile assumes someone has *created* a skeptic profile in their ~/.codex/config.toml. Mapping to a Gemini extension assumes an extension exists that provides skeptical analysis.

This creates a false equivalence. Users will expect consistent behavior across providers, but the underlying mechanisms are so different that the same 'persona' flag would produce wildly different results - or silently fail.

Better to be honest about the differences and let users learn provider-specific options than to paper over them with a leaky abstraction.

### 2026-01-24T20:17:29.849Z - Alternative
ALTERNATIVE: Inject Style via System Prompt (Provider-Agnostic)

What if --style works universally by prepending a system prompt to the query, rather than relying on provider-specific mechanisms?

```bash
memory think add 'Should we use Redis?' --call codex --style Devils-Advocate --model o3
```

Behind the scenes:
1. Plugin reads style definition from styles/Devils-Advocate.md
2. Constructs prompt: 'You are playing devil\'s advocate. Challenge assumptions... \n\nUser thought: Should we use Redis?'
3. Calls: `codex exec "<constructed prompt>" --model o3`

This works because:
- All three CLIs accept arbitrary prompts
- Style is just prompt engineering, not a provider feature
- No dependency on provider-specific config

Limitations:
- --agent still won't work (that requires Claude Code's subagent system)
- Quality of style adherence may vary by model
- Longer prompts = more tokens

This gives us: universal --model + universal --style, provider-specific everything else.

### 2026-01-24T20:18:14.507Z - Thought (model:haiku style:Pragmatist [d0a1b864-10d2-4533-867a-71b95f23198b])
Thought recorded to the deliberation. I've added a critical analysis that challenges the prompt injection approach by examining four key friction points:

1. **Execution quality degradation** — system prompts can't replicate the deep architectural optimisations built into each provider's native agent/style systems
2. **Maintenance burden scaling** — keeping style descriptions consistent across three fundamentally different provider architectures becomes expensive
3. **Misaligned user expectations** — users don't actually want provider-agnostic results; they pick a provider and stay loyal to it
4. **Hidden complexity** — prompt injection makes the problem "work" in happy paths but masks fundamental incompatibilities that surface later

The key insight is that this isn't really about implementation complexity—it's about **fidelity vs. maintainability tradeoff**. Option C (explicit abstraction layer) wins if you need >85% fidelity. Option B (prompt injection) only wins if you're comfortable with degraded results in exchange for implementation simplicity, which defeats the purpose since users will notice the quality drop immediately.

This grounds the deliberation in a concrete decision criterion rather than abstract elegance.

### 2026-01-24T20:19:01.138Z - Thought
PROOF OF CONCEPT: Cross-provider calling works!

Tested both Codex and Gemini with a deliberation prompt. Results:

**Codex (gpt-5-codex):**
> Cross-provider agent calling would widen compatibility, but it also multiplies trust, versioning, and quota-management complexity—each provider's semantics and limits leak into the UX. If we do it, we'd need a strong abstraction layer plus capability detection so users only see features that actually work for their configured providers.

**Gemini (gemini-2.5-pro):**
> Supporting cross-provider agents would grant users immense flexibility, allowing them to leverage the best model for any given task without being locked into a single ecosystem. However, this introduces significant architectural complexity and maintenance overhead, as we would need to build and sustain a robust abstraction layer over disparate APIs. We must carefully weigh the value of this provider-agnosticism against the risk of diluting our focus and creating a less cohesive user experience.

**Observations:**
1. Both CLIs accept plain prompts and return usable text output
2. Response quality is comparable to Claude for this use case
3. Both ironically warned about the complexity of the very feature we're contemplating
4. Command structure differs: `codex exec "prompt" --model X` vs `gemini "prompt" --model X -o text`
5. Gemini has startup noise (extension loading) that may need filtering

### 2026-01-24T20:19:13.711Z - Thought
RECOMMENDED IMPLEMENTATION for v1.1.0:

Based on CLI exploration and proof-of-concept testing, here's a pragmatic approach:

**Tier 1: Universal Parameters (all providers)**
- `--model`: Maps directly to each provider's -m flag
- `--style`: Inject via prompt prefix (works everywhere, quality may vary)

**Tier 2: Provider-Specific Parameters (opt-in)**
- `--call claude --agent X`: Only works with Claude (native subagent system)
- `--call codex --profile X`: Only works with Codex (requires user config)
- `--call gemini --extensions X`: Only works with Gemini (plugin system)

**Implementation Details:**
```typescript
interface ProviderConfig {
  claude: { command: 'claude --print', modelFlag: '--model', supportsAgent: true, supportsStyle: true };
  codex: { command: 'codex exec', modelFlag: '--model', supportsProfile: true };
  gemini: { command: 'gemini', modelFlag: '--model', outputFlag: '-o text', supportsExtensions: true };
}
```

**Error Handling:**
- If user passes --agent with codex/gemini: warn and ignore
- If user passes --profile with claude/gemini: error with suggestion
- If provider CLI not found: graceful error with install instructions

**Output Parsing:**
- Codex: Strip header lines (version, workdir, etc.), extract final response
- Gemini: Filter extension loading noise, extract main output
- Claude: Use --print mode for clean output

### 2026-01-24T20:22:42.434Z - Thought
Benchmarking reality check for --auto:
- gemma3:1b: ~3s but outputs garbage ({'style': 'Haiku', 'model': 'Redis'} - completely wrong)
- gemma3:4b: ~10s but correct output ({'style': 'Pragmatist', 'model': 'sonnet'})

The 2s timeout in the spec is fantasy. Real options:
1. Accept 10s latency with spinner (user knows it's thinking)
2. Configurable model in memory.local.md (fast GPU users can use bigger models)
3. Pre-compute: run selection when 'think create' called, cache for subsequent 'add' commands
4. Heuristic fallback: keyword matching instead of LLM (security→security-auditor, etc)
5. Drop --auto entirely - maybe it's not worth the complexity

The heuristic approach is actually compelling - deterministic, instant, no model dependency.

### 2026-01-24T20:40:18.950Z - Thought
USER INPUT: Defer to v1.2.0

User decision: Cross-provider calling should be a v1.2.0 addition, keeping implementation minimal.

**v1.1.0 Scope (Minimal):**
- Support `--call codex` and `--call gemini` 
- Only `--model` parameter is portable
- Fail gracefully if CLI not installed

**Additional Context from User:**
- Codex supports `--oss` mode for local models (no subscription required):
  - `--model gpt:oss-20b` or `--model gpt:oss-120b` with `--oss` flag
  - Requires powerful hardware (user reports ~1 token/s on gpt:oss-20b)
- Gemini does not currently support local models
  - May change if Gemma gets tool-call support and CLI integration
- Both require subscriptions/API keys for cloud models

**Implementation for v1.1.0:**
```bash
# Cloud models (require subscription)
memory think add 'Topic' --call codex --model gpt-5-codex
memory think add 'Topic' --call gemini --model gemini-2.5-pro

# Local models (Codex only, no subscription)
memory think add 'Topic' --call codex --model gpt:oss-20b --oss
```

**v1.2.0 Future Work:**
- Add --style via prompt injection
- Add provider-specific params (--profile, --extensions, --agent)
- Explore Gemma local model support if available

### 2026-01-24T20:40:26.846Z - Conclusion
v1.1.0 will add basic cross-provider support: --call codex and --call gemini with --model as the only portable parameter. Codex also supports --oss flag for local models (gpt:oss-20b/120b). CLIs must be installed or command fails gracefully. Full feature set (--style via prompt injection, provider-specific params like --profile/--extensions/--agent) deferred to v1.2.0. This keeps v1.1.0 scope manageable while proving the multi-provider concept.
